### 使用上下文

在这个数据集上，我们被要求，对若干个句子之中的特定一句输出其隐式情感。非常自然的，我们假定目标句的上下文为目标句提供了额外信息，并且基于这个假想进行了一些实验。

**朴素拼接**

我们尝试将目标句，以及其上下文进行拼接，作为一个更大的整体来判断情感。但是，在几次实验当中，这个方案效果不大，并且有时有着比仅使用目标句更为糟糕的表现。通过观察错误样例，我们认为这可能是因为邻近句子并不总是和目标句保持相近的情感所导致的。

**test_a和test_b**

受到了[Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence](https://www.aclweb.org/anthology/N19-1035)的启发，我们也尝试了一种类似于构造句子的做法。我们将拼接了上下文的长句子作为test_a，将目标句作为test_b，期待能达到询问*在该上下文中，目标句的情感是什么*的效果。相较于仅使用目标句，该方法在恰当的参数设置下，在dev集上能提高1%左右的正确率。

**添加前缀后缀的多句子拼接**

不加任何分隔符的拼接方案，在bert模型上的实现，终究是只取出最后一层隐状态的第一个位置，进行后续处理得到预测的分类。从bert的模型设计而言，该向量表达的是输入序列整体上的分类倾向，而不是一段上下文之中，我们所要的目标句的分类倾向。

为了解决这个问题，我们参考了[Fine-tune BERT for Extractive Summarization](https://arxiv.org/abs/1903.10318)的做法，在每个句子的前后分别添上\[CLS]和\[SEP]标记，然后再进行拼接。在计算分类时，取出目标的\[CLS]对应的向量来计算。相较于test_a+test_b的方法，在dev集上能再提升约0.7%正确率。

### 处理过拟合

我们在实验中发现，模型在train数据集上的正确率能非常快速的提升，最终往往能轻易达到97%以上，但在dev集上随epoch增加，提升非常低。同时，我们观察到，相当一部分的错误样例，在错误的分类上有着惊人的、过90%的预测概率。我们认为模型可能存在着过拟合的问题。

我们参考了[Unsupervised Data Augmentation for Consistency Training](https://arxiv.org/abs/1904.12848)的做法。在每一个batch当中，如果一个example能被正确预测，并且预测概率超过了设定阈值，那么在loss回传并更新参数时，这个example就不被纳入计算。这个方法能让模型更多的在划分错误的example上进行学习。在dev集上，使用该技巧后，正确率提升约0.4%。
