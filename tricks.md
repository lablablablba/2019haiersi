### 语言模型预训练

- 在一开始的实验中我们发现，百度的ernie会比bert_chinese_base效果好，经过查阅资料，我们得知果用同domain相近的语料进行语言模型预训练，可以提升模型效果
，因此我们先后用ernie在训练集、验证集、网上搜索的微博语料、以及最终的测试集上进行了语言模型预训练，后续实验表明预训练确实会带来一定的提升

### 半监督学习

- 实验过程中我们发现模型在训练集上模型发生了严重拟合，在训练集上的准确率一度达到97%，再加上通过分析错误样例，发现模型容易被个词语所误导，而不是关心整个句
子本身，因此我们开始了增加模型泛化性的实验。受到[Unsupervised Data Augmentation](https://github.com/google-research/uda)的启发，我们使用了tsa和vat
两项半监督学习技术。
### VAT
- 在训练过程中，以20%的几率mask掉一些字，然后算扰动后句子的输出和没有扰动的句子的输出之间的kl上散度，将它加入到原本的loss中作为最终的loss，
以此增强模型的抗干扰能力

- 实验表明使用这两种技术确实会给模型带来提升，在accuracy这项指标上可以得到1%左右的提升


### 模型多样性
- **到目前为止，使用以上方法，只用bert+线性层，已经能在验证集上达到84%的表现，如果最后一层换上bilstm+attention，可以达到84.6%，为了后续的模型集成，
我们决定训练***尽可能不一样***的模型来减少模型输出间的相关性，从而增加集成的性能**

  + bert和不同模型的堆叠:我们分别使用了bert+线性层和bert+bilstm+attention

  + max_length：实验过程中发现max_length会影响表现，最后我们分别使用120，160，200的长度来训练模型

  + 上下文：

  + 增加类别的权重：实验中我们发现积极类别的召回率低很多，因此我们又训练了一个在积极类别上表现比较好的模型，方法在计算loss的时候增加积极类别的权重
